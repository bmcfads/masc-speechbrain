# ############################################################################
# Tokenizer: Unigram 50
# Training: STOP
# Authors:  Brian McFadden 2024
#   (original: Timers-and-Such - Abdel Heba 2021)
# ############################################################################

output_folder: results/tokenizer-unigram/
# train_log: !ref <output_folder>/train_log.txt

# Data files
data_folder: !PLACEHOLDER # e.g, /localscratch/stop
train_csv: !ref <output_folder>/train-type=direct.csv
valid_csv: !ref <output_folder>/eval-type=direct.csv
skip_prep: False

####################### Training Parameters ####################################
token_type: unigram  # ["unigram", "bpe", "char"]
token_output: 50  # required chars = 44
character_coverage: 1.0
bos_id: 1  # <s>
eos_id: 2  # </s>
csv_read: semantics

tokenizer: !name:speechbrain.tokenizers.SentencePiece.SentencePiece
   model_dir: !ref <output_folder>
   vocab_size: !ref <token_output>
   annotation_train: !ref <train_csv>
   annotation_read: !ref <csv_read>
   model_type: !ref <token_type>
   character_coverage: !ref <character_coverage>
   bos_id: !ref <bos_id>
   eos_id: !ref <eos_id>
   annotation_list_to_check: [!ref <train_csv>, !ref <valid_csv>]
